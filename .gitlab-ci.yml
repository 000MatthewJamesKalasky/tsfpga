# Instructions here: https://docs.gitlab.com/ee/ci/yaml/
# Linter here: https://gitlab.com/tsfpga/tsfpga/-/ci/lint

stages:
  - test
  - build_documentation
  - deploy


workflow:
  rules:
    # Run pipeline for scheduled (nightly master runs), for merge requests, and
    # when triggered manually on the web.
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_PIPELINE_SOURCE == "schedule"'
    - if: '$CI_PIPELINE_SOURCE == "web"'
    # Pushes to master (a successful merge request is also a "push") should run so
    # that new website is deployed.
    - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == "master"'
    # PyPI deploy should be run only when there is a tag. However a $CI_COMMIT_TAG is
    # set only on the $CI_PIPELINE_SOURCE = "push" event (not on merge_request_event
    # or schedule). So allow the pipeline to run for this as well.
    - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_TAG != null'


default:
  # A running job should be canceled if made redundant by a newer pipeline run.
  interruptible: true
  # Use docker image from ghdl project
  # Available at https://hub.docker.com/r/ghdl/vunit/
  # Configured at https://github.com/ghdl/docker/
  # Note that the mcode image is much smaller than the gcc image.
  image: ghdl/vunit:mcode-master
  before_script:
    - export PYTHONPATH=$(pwd)/tsfpga
    - echo $CI_COMMIT_TAG
    - echo $CI_COMMIT_BRANCH
    - echo $CI_PIPELINE_SOURCE
    - echo $CI_COMMIT_REF_NAME


pytest:
  stage: test
  script:
    - apt-get update -qq > /dev/null
    - apt-get install -y -qq git g++ > /dev/null
    - python3 -m pip install --requirement requirements_develop.txt > /dev/null
    - python3 -m pylint --version
    - python3 -m pycodestyle --version
    - python3 -m pytest -v --cov tsfpga --cov-report xml:generated/python_coverage.xml --cov-report html:generated/python_coverage_html
      --ignore=tsfpga/test/functional/vivado --ignore=tsfpga/test/functional/commercial_simulators
      tsfpga
  artifacts:
    paths:
      - generated/python_coverage.xml
      - generated/python_coverage_html


simulate:
  stage: test
  script:
    - apt-get update -qq > /dev/null
    - apt-get install -y -qq git > /dev/null
    - python3 -m pip install GitPython tomlkit > /dev/null
    - ghdl --version
    # Run minimal simulation subset for merge requests
    - if [ $CI_PIPELINE_SOURCE == "merge_request_event" ]; then export SIMULATE_FLAGS="--git-minimal"; fi;
    - python3 examples/simulate.py --num-threads 4 --vivado-skip $SIMULATE_FLAGS


build_pypi:
  stage: test
  script:
    - apt-get update -qq > /dev/null
    - apt-get install -y -qq git > /dev/null
    - python3 -m pip install GitPython tomlkit > /dev/null
    - python3 setup.py sdist
  artifacts:
    paths:
      - dist


formal:
  stage: test
  image: tsfpga/formal
  script:
    - ghdl --version
    # We need a newer version of axi_stream_pkg.vhd than is currently included in the docker image
    - git clone --recurse-submodules --depth 1 --single-branch --branch master https://github.com/VUnit/vunit.git ../vunit
    - python3 examples/formal.py --num-threads 4


build_pages:
  # Uses coverage artifacts from the pytest job in the previous stage
  stage: build_documentation
  # Start as soon as pytest job is done. Even if e.g. simaulate in previous stage is not.
  needs: ["pytest"]
  script:
    - apt-get update -qq > /dev/null
    - apt-get install -y -qq git graphviz > /dev/null
    - python3 -m pip install --requirement requirements_develop.txt > /dev/null
    - python3 tools/build_docs.py $DOC_FLAGS
  artifacts:
    paths:
      - generated/sphinx_html


deploy_pypi:
  # Deploy artifacts from the build_pypi job run in the previous stage
  stage: deploy
  rules:
    - if: '$CI_COMMIT_TAG != null'
  variables:
    TWINE_USERNAME: __token__
    TWINE_PASSWORD: $PYPI_API_TOKEN
  script:
    - apt-get update -qq > /dev/null
    - apt-get install -y -qq git > /dev/null
    - python3 -m pip install twine > /dev/null
    - python3 -m twine upload dist/*


pages:
  # Job name "pages" is magic in gitlab. Will deploy content of the "public" folder to the website.
  # Uses artifacts from the build_pages job run in the previous stage.
  stage: deploy
  # Use a minimal image, so no CI time is wasted fetching a larger image. This step does not need
  # anything special.
  image: alpine
  rules:
    - if: '$CI_COMMIT_BRANCH == "master"'
  script:
    - mv generated/sphinx_html public
  artifacts:
    paths:
      - public
