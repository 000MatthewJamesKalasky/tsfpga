# Instructions here: https://docs.gitlab.com/ee/ci/yaml/
# Linter here: https://gitlab.com/tsfpga/tsfpga/-/ci/lint

stages:
  - test
  - build_documentation
  - deploy


workflow:
  rules:
    # Run pipeline for scheduled (nightly master runs) and for merge requests.
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event"'
    - if: '$CI_PIPELINE_SOURCE == "schedule"'
    # Pushes to master (a succesful merge request is also a "push") should run so
    # that new website is deployed.
    - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == "master"'
    # PyPI deply should be run only when there is a tag. However a $CI_COMMIT_TAG is
    # set only on the $CI_PIPELINE_SOURCE = "push" event (not on merge_request_event
    # or schedule). So allow the pipeline to run for this as well.
    - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_TAG != null'


default:
  # Use docker image from ghdl project
  # Available at https://hub.docker.com/r/ghdl/vunit/
  # Configured at https://github.com/ghdl/docker/tree/master/dockerfiles
  image: ghdl/vunit:gcc-master
  before_script:
    - python3 --version
    - export PYTHONPATH=$(pwd)/tsfpga
    - echo $CI_COMMIT_TAG
    - echo $CI_COMMIT_BRANCH
    - echo $CI_PIPELINE_SOURCE
    - echo $CI_COMMIT_REF_NAME


pytest:
  stage: test
  script:
    - apt-get update -qq > /dev/null
    - apt-get install -y -qq git g++ > /dev/null
    # Install wheel first, so that wrapt is installed correctly afterwards
    - python3 -m pip install wheel > /dev/null
    - python3 -m pip install --requirement requirements_develop.txt > /dev/null
    - python3 -m pylint --version
    - python3 -m pycodestyle --version
    - python3 -m pytest -v --cov tsfpga --cov-report xml:generated/python_coverage.xml --cov-report html:generated/python_coverage_html --ignore=tsfpga/test/functional/vivado tsfpga
  artifacts:
    paths:
      - generated/python_coverage.xml
      - generated/python_coverage_html


simulate:
  stage: test
  script:
    - python3 -m pip install --requirement requirements_develop.txt > /dev/null
    - ghdl --version
    - python3 examples/simulate.py --num-threads 4 --vivado-skip
  artifacts:
    paths:
      - generated/vhdl_coverage.xml
      - generated/vhdl_coverage_html


build_pypi:
  stage: test
  script:
    - apt-get update -qq > /dev/null
    - apt-get install -y -qq git > /dev/null
    - python3 setup.py sdist
  artifacts:
    paths:
      - dist


build_pages:
  # Uses coverage artifacts from the pytest and simulate jobs in the previous stage
  stage: build_documentation
  script:
    - apt-get update -qq > /dev/null
    - apt-get install -y -qq git > /dev/null
    - python3 -m pip install --requirement requirements_develop.txt > /dev/null
    - python3 tools/build_docs.py
  artifacts:
    paths:
      - generated/sphinx_html


deploy_pypi:
  # Deploy artifacts from the build_pypi job run in the previous stage
  stage: deploy
  rules:
    - if: '$CI_COMMIT_TAG != null'
  variables:
    TWINE_USERNAME: __token__
    TWINE_PASSWORD: $PYPI_API_TOKEN
  script:
    - apt-get update -qq > /dev/null
    - apt-get install -y -qq git > /dev/null
    - python3 -m pip install --requirement requirements_develop.txt > /dev/null
    - python3 -m twine upload dist/*


pages:
  # Job name "pages" is magic in gitlab. Will deploy content of the "public" folder to the website.
  # Uses artifacts from the build_pages job run in the previous stage.
  stage: deploy
  rules:
    - if: '$CI_COMMIT_BRANCH == "master"'
  script:
    - mv generated/sphinx_html public
  artifacts:
    paths:
      - public
